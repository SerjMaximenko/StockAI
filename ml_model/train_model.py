import time
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from kafka import KafkaConsumer
import json
from sklearn.preprocessing import MinMaxScaler
import os

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Kafka
KAFKA_BROKER = "localhost:9092"
TOPIC_NAME = "historical-data"

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
SEQUENCE_LENGTH = 50
EPOCHS = 200
BATCH_SIZE = 16
LEARNING_RATE = 0.001

# –°–æ–∑–¥–∞—ë–º LSTM-–º–æ–¥–µ–ª—å
class StockLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(StockLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Kafka
def load_data_from_kafka():
    consumer = KafkaConsumer(
        TOPIC_NAME,
        bootstrap_servers=KAFKA_BROKER,
        auto_offset_reset='earliest',
        enable_auto_commit=False,
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )

    data = []
    timeout = 10  # –û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö 10 —Å–µ–∫—É–Ω–¥
    start_time = time.time()

    while time.time() - start_time < timeout:
        msg = consumer.poll(timeout_ms=1000)
        if msg:
            for _, record in msg.items():
                for message in record:
                    data.append(message.value)

    consumer.close()

    if not data:
        print("‚ö† –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ Kafka! –ü—Ä–æ–≤–µ—Ä—å `historical-data`.")
        exit(1)

    return pd.DataFrame(data)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
def prepare_data(df):
    df = df.copy()  # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å `SettingWithCopyWarning`
    df.sort_values("timestamp", inplace=True)

    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df[["open_price", "high_price", "low_price", "close_price", "volume"]])

    X, y = [], []
    for i in range(len(scaled_data) - SEQUENCE_LENGTH):
        X.append(scaled_data[i:i + SEQUENCE_LENGTH])
        y.append(scaled_data[i + SEQUENCE_LENGTH, 3])  # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è - —Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è

    return np.array(X), np.array(y), scaler

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
def train_model():
    print("üì° –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Kafka...")
    df = load_data_from_kafka()

    unique_symbols = df["symbol"].unique()  # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(unique_symbols)} –∞–∫—Ü–∏–π: {list(unique_symbols)}")

    for symbol in unique_symbols:
        print(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {symbol}...")

        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–∏–º–≤–æ–ª–∞
        df_symbol = df[df["symbol"] == symbol]
        if df_symbol.shape[0] < SEQUENCE_LENGTH:
            print(f"‚ö† –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        X, y, scaler = prepare_data(df_symbol)
        X_train = torch.tensor(X, dtype=torch.float32).to(device)
        y_train = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(device)

        # –°–æ–∑–¥–∞—ë–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        model = StockLSTM(input_size=5, hidden_size=200, num_layers=5, output_size=1).to(device)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

        for epoch in range(EPOCHS):
            model.train()
            optimizer.zero_grad()
            outputs = model(X_train)
            loss = criterion(outputs, y_train)
            loss.backward()
            optimizer.step()

            if (epoch + 1) % 50 == 0:
                print(f"üü¢ {symbol} Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.6f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–∏–º–≤–æ–ª–∞
        os.makedirs("models", exist_ok=True)
        model_path = f"models/stock_lstm_{symbol}.pth"
        torch.save(model.state_dict(), model_path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è {symbol} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

if __name__ == "__main__":
    train_model()