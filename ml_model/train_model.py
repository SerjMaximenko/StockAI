import time

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from kafka import KafkaConsumer
import json
from sklearn.preprocessing import MinMaxScaler
import os

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Kafka
KAFKA_BROKER = "localhost:9092"
TOPIC_NAME = "historical-data"

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
SEQUENCE_LENGTH = 50
EPOCHS = 200
BATCH_SIZE = 16
LEARNING_RATE = 0.001

# –°–æ–∑–¥–∞—ë–º Kafka Consumer –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
def load_data_from_kafka():
    consumer = KafkaConsumer(
        TOPIC_NAME,
        bootstrap_servers=KAFKA_BROKER,
        auto_offset_reset='earliest',
        enable_auto_commit=False,
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )

    data = []
    timeout = 10  # –û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö 10 —Å–µ–∫—É–Ω–¥
    start_time = time.time()

    while time.time() - start_time < timeout:
        msg = consumer.poll(timeout_ms=1000)
        if msg:
            for _, record in msg.items():
                for message in record:
                    data.append(message.value)

    consumer.close()

    if not data:
        print("‚ö† –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ Kafka! –ü—Ä–æ–≤–µ—Ä—å `historical-data`.")
        exit(1)

    return pd.DataFrame(data)

# –°–æ–∑–¥–∞—ë–º LSTM-–º–æ–¥–µ–ª—å
class StockLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(StockLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è LSTM
def prepare_data(df):
    df.sort_values("timestamp", inplace=True)
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df[["open_price", "high_price", "low_price", "close_price", "volume"]])

    X, y = [], []
    for i in range(len(scaled_data) - SEQUENCE_LENGTH):
        X.append(scaled_data[i:i + SEQUENCE_LENGTH])
        y.append(scaled_data[i + SEQUENCE_LENGTH, 3])  # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è - —Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è

    return np.array(X), np.array(y), scaler

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def train_model():
    print("üì° –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Kafka...")
    df = load_data_from_kafka()

    print("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    X, y, scaler = prepare_data(df)

    X_train = torch.tensor(X, dtype=torch.float32)
    y_train = torch.tensor(y, dtype=torch.float32).view(-1, 1)

    model = StockLSTM(input_size=5, hidden_size=70, num_layers=3, output_size=1)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    for epoch in range(EPOCHS):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 5 == 0:
            print(f"üü¢ Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.6f}")

    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
    os.makedirs("models", exist_ok=True)
    torch.save(model.state_dict(), "models/stock_lstm.pth")
    print("üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ models/stock_lstm.pth")

if __name__ == "__main__":
    train_model()
